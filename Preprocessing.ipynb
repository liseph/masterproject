{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import reverse_geocoder as rg\n",
    "import preprocessor as p\n",
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "import math\n",
    "from multiprocessing import  Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/tweetsreplies4.tsv', encoding='cp1252', sep=\"\\t\", usecols=['timestamp_ms','longitude', 'latitude', 'text', 'lang'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Remove rows with missing text, filter out non-english tweets. Get detailed locations and timestamp and tokenize the text.\n",
    "* Fetch more location info from longitude and latitude using reverse_encoder https://github.com/thampiman/reverse-geocoder\n",
    "* Convert ms timestamps to datetime object.\n",
    "* Preprocess text using the preprocessor https://github.com/s/preprocessor and remove stopwords using gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(585341+536) # this row makes the function p.clean crash...\n",
    "df = df.drop(df[df.lang != 'en'].index)\n",
    "df = df.drop(['lang'], axis=1)\n",
    "df = df.drop(df[df.text == ''].index)\n",
    "df = df.drop(df[df.text.isna()].index)\n",
    "df = df.reset_index()\n",
    "df = df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading formatted geocoded file...\n"
     ]
    }
   ],
   "source": [
    "coordinates = list(df[['latitude','longitude']].itertuples(index=False, name=None))\n",
    "locations = rg.search(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_df = pd.json_normalize(locations)[['name', 'admin1', 'admin2', 'cc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, locations_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(df[df.cc != 'US'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func, n_cores=4):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.NUMBER)\n",
    "def add_features(df):\n",
    "    try:\n",
    "        df['text'] = df['text'].apply(p.clean)\n",
    "    except:\n",
    "        print(df['text'])\n",
    "    return df\n",
    "df = parallelize_dataframe(df, add_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make lower case and remove numbers and white space, must be done after cleaning\n",
    "df['text']  = df['text'] \\\n",
    "                .str.lower() \\\n",
    "                .str.replace('\\d+', '') \\\n",
    "                .str.replace('a{2,}', 'a') \\\n",
    "                .str.replace('b{3,}', 'b') \\\n",
    "                .str.replace('c{3,}', 'c') \\\n",
    "                .str.replace('d{3,}', 'd') \\\n",
    "                .str.replace('e{3,}', 'e') \\\n",
    "                .str.replace('f{3,}', 'f') \\\n",
    "                .str.replace('g{3,}', 'g') \\\n",
    "                .str.replace('h{3,}', 'h') \\\n",
    "                .str.replace('i{2,}', 'i') \\\n",
    "                .str.replace('j{3,}', 'j') \\\n",
    "                .str.replace('k{3,}', 'k') \\\n",
    "                .str.replace('l{3,}', 'l') \\\n",
    "                .str.replace('m{3,}', 'm') \\\n",
    "                .str.replace('n{3,}', 'n') \\\n",
    "                .str.replace('o{3,}', 'o') \\\n",
    "                .str.replace('p{3,}', 'p') \\\n",
    "                .str.replace('q{3,}', 'q') \\\n",
    "                .str.replace('r{3,}', 'r') \\\n",
    "                .str.replace('s{3,}', 's') \\\n",
    "                .str.replace('t{3,}', 't') \\\n",
    "                .str.replace('u{2,}', 'u') \\\n",
    "                .str.replace('v{3,}', 'v') \\\n",
    "                .str.replace('w{3,}', 'w') \\\n",
    "                .str.replace('x{3,}', 'x') \\\n",
    "                .str.replace('y{2,}', 'y') \\\n",
    "                .str.replace('z{3,}', 'z') \\\n",
    "                .str.replace('_', ' ') \\\n",
    "                .str.replace(' rt ', '') \\\n",
    "                .str.replace('#', '') \\\n",
    "                .str.replace('[^\\w\\s]',' ') \\\n",
    "                .str.replace('\\s\\s+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords, must be done after cleaning and removal of white space\n",
    "def remove_stopwords_f(df):\n",
    "    df['text'] = df['text'].apply(remove_stopwords)\n",
    "    return df\n",
    "df = parallelize_dataframe(df, remove_stopwords_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words with less than 3 characters, should be done after removing stop words\n",
    "remove_short_words = lambda x: ' '.join([item for item in x.split(\" \") if len(item) > 2])\n",
    "def remove_short_words_f(df):\n",
    "    df['text'] = df['text'].apply(remove_short_words)\n",
    "    return df\n",
    "df = parallelize_dataframe(df, remove_short_words_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>text</th>\n",
       "      <th>name</th>\n",
       "      <th>admin1</th>\n",
       "      <th>admin2</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1443887442007</td>\n",
       "      <td>-66.578926</td>\n",
       "      <td>6.422820</td>\n",
       "      <td>quiero que sea sponsor zayn malik goals rts sa...</td>\n",
       "      <td>Puerto Carreno</td>\n",
       "      <td>Vichada</td>\n",
       "      <td></td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1443888522674</td>\n",
       "      <td>-73.948775</td>\n",
       "      <td>40.655138</td>\n",
       "      <td>wouldn analogy sense went church eat christians</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>New York</td>\n",
       "      <td>Kings County</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1443887588840</td>\n",
       "      <td>-122.630908</td>\n",
       "      <td>45.536402</td>\n",
       "      <td>going taco bell absolutely makes taco</td>\n",
       "      <td>Portland</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>Multnomah County</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1443887443914</td>\n",
       "      <td>-73.948775</td>\n",
       "      <td>40.655138</td>\n",
       "      <td>confirmed justin bieber incredibly dumb</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>New York</td>\n",
       "      <td>Kings County</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1443891813050</td>\n",
       "      <td>-2.298134</td>\n",
       "      <td>52.847090</td>\n",
       "      <td>aww thank gillian olly follow thank</td>\n",
       "      <td>Eccleshall</td>\n",
       "      <td>England</td>\n",
       "      <td>Staffordshire</td>\n",
       "      <td>GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1444214110880</td>\n",
       "      <td>88.352242</td>\n",
       "      <td>22.612706</td>\n",
       "      <td>favu bcoz knowv sidehero</td>\n",
       "      <td>Chakapara</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>Haora</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1444214385820</td>\n",
       "      <td>72.570232</td>\n",
       "      <td>23.013959</td>\n",
       "      <td>current state amethi best example hard work</td>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>Ahmadabad</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1444214711324</td>\n",
       "      <td>72.570232</td>\n",
       "      <td>23.013959</td>\n",
       "      <td>people alleged development work took place vil...</td>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>Ahmadabad</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1444214681705</td>\n",
       "      <td>72.570232</td>\n",
       "      <td>23.013959</td>\n",
       "      <td>people darkha village comes amethi decided boy...</td>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>Ahmadabad</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1444214734446</td>\n",
       "      <td>72.570232</td>\n",
       "      <td>23.013959</td>\n",
       "      <td>rahul gandhi time constituency</td>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>Ahmadabad</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7474 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp_ms   longitude   latitude  \\\n",
       "1     1443887442007  -66.578926   6.422820   \n",
       "3     1443888522674  -73.948775  40.655138   \n",
       "4     1443887588840 -122.630908  45.536402   \n",
       "5     1443887443914  -73.948775  40.655138   \n",
       "8     1443891813050   -2.298134  52.847090   \n",
       "...             ...         ...        ...   \n",
       "9995  1444214110880   88.352242  22.612706   \n",
       "9996  1444214385820   72.570232  23.013959   \n",
       "9997  1444214711324   72.570232  23.013959   \n",
       "9998  1444214681705   72.570232  23.013959   \n",
       "9999  1444214734446   72.570232  23.013959   \n",
       "\n",
       "                                                   text            name  \\\n",
       "1     quiero que sea sponsor zayn malik goals rts sa...  Puerto Carreno   \n",
       "3       wouldn analogy sense went church eat christians        Brooklyn   \n",
       "4                 going taco bell absolutely makes taco        Portland   \n",
       "5               confirmed justin bieber incredibly dumb        Brooklyn   \n",
       "8                   aww thank gillian olly follow thank      Eccleshall   \n",
       "...                                                 ...             ...   \n",
       "9995                           favu bcoz knowv sidehero       Chakapara   \n",
       "9996        current state amethi best example hard work       Ahmedabad   \n",
       "9997  people alleged development work took place vil...       Ahmedabad   \n",
       "9998  people darkha village comes amethi decided boy...       Ahmedabad   \n",
       "9999                     rahul gandhi time constituency       Ahmedabad   \n",
       "\n",
       "           admin1            admin2  cc  \n",
       "1         Vichada                    CO  \n",
       "3        New York      Kings County  US  \n",
       "4          Oregon  Multnomah County  US  \n",
       "5        New York      Kings County  US  \n",
       "8         England     Staffordshire  GB  \n",
       "...           ...               ...  ..  \n",
       "9995  West Bengal             Haora  IN  \n",
       "9996      Gujarat         Ahmadabad  IN  \n",
       "9997      Gujarat         Ahmadabad  IN  \n",
       "9998      Gujarat         Ahmadabad  IN  \n",
       "9999      Gujarat         Ahmadabad  IN  \n",
       "\n",
       "[7474 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_short_texts_f(df):\n",
    "    df['c'] = df['text'].apply(lambda x: len(x.split(\" \")))\n",
    "    df = df.drop(df[df.c < 3].index)\n",
    "    df = df.drop(['c'], axis=1)\n",
    "    return df\n",
    "\n",
    "# Remove tweets with less than 3 words.\n",
    "df = parallelize_dataframe(df, remove_short_texts_f)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df.text == ''].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['timestamp_ms', 'admin1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['admin2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>text</th>\n",
       "      <th>name</th>\n",
       "      <th>admin1</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1443887442007</td>\n",
       "      <td>-66.578926</td>\n",
       "      <td>6.422820</td>\n",
       "      <td>quiero que sea sponsor zayn malik goals rts sa...</td>\n",
       "      <td>Puerto Carreno</td>\n",
       "      <td>Vichada</td>\n",
       "      <td>CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1443887443914</td>\n",
       "      <td>-73.948775</td>\n",
       "      <td>40.655138</td>\n",
       "      <td>confirmed justin bieber incredibly dumb</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>New York</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1443887588840</td>\n",
       "      <td>-122.630908</td>\n",
       "      <td>45.536402</td>\n",
       "      <td>going taco bell absolutely makes taco</td>\n",
       "      <td>Portland</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1443888294480</td>\n",
       "      <td>-100.076888</td>\n",
       "      <td>31.168893</td>\n",
       "      <td>atlanta backwards atlanta</td>\n",
       "      <td>Eden</td>\n",
       "      <td>Texas</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1443888510207</td>\n",
       "      <td>-89.266507</td>\n",
       "      <td>39.739300</td>\n",
       "      <td>republican strategist actually said morning ch...</td>\n",
       "      <td>Edinburg</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>1443929955976</td>\n",
       "      <td>-79.980689</td>\n",
       "      <td>40.431389</td>\n",
       "      <td>currently mario shadyside handing koozies</td>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>1443930099195</td>\n",
       "      <td>-82.388215</td>\n",
       "      <td>36.344668</td>\n",
       "      <td>thanks appreciate makeadifference addvalue tod...</td>\n",
       "      <td>Johnson City</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>1443930134934</td>\n",
       "      <td>-117.434222</td>\n",
       "      <td>47.667438</td>\n",
       "      <td>shit pants stage stay tuned</td>\n",
       "      <td>Spokane</td>\n",
       "      <td>Washington</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>1443930511894</td>\n",
       "      <td>-100.076888</td>\n",
       "      <td>31.168893</td>\n",
       "      <td>okay astros fans weve torn rooting agree want ...</td>\n",
       "      <td>Eden</td>\n",
       "      <td>Texas</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>1443930513563</td>\n",
       "      <td>-80.396670</td>\n",
       "      <td>32.761775</td>\n",
       "      <td>sentiment exactly maineloves mainemendozaonkmjs</td>\n",
       "      <td>Ravenel</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timestamp_ms   longitude   latitude  \\\n",
       "1     1443887442007  -66.578926   6.422820   \n",
       "5     1443887443914  -73.948775  40.655138   \n",
       "4     1443887588840 -122.630908  45.536402   \n",
       "12    1443888294480 -100.076888  31.168893   \n",
       "18    1443888510207  -89.266507  39.739300   \n",
       "...             ...         ...        ...   \n",
       "809   1443929955976  -79.980689  40.431389   \n",
       "229   1443930099195  -82.388215  36.344668   \n",
       "1696  1443930134934 -117.434222  47.667438   \n",
       "1707  1443930511894 -100.076888  31.168893   \n",
       "1690  1443930513563  -80.396670  32.761775   \n",
       "\n",
       "                                                   text            name  \\\n",
       "1     quiero que sea sponsor zayn malik goals rts sa...  Puerto Carreno   \n",
       "5               confirmed justin bieber incredibly dumb        Brooklyn   \n",
       "4                 going taco bell absolutely makes taco        Portland   \n",
       "12                            atlanta backwards atlanta            Eden   \n",
       "18    republican strategist actually said morning ch...        Edinburg   \n",
       "...                                                 ...             ...   \n",
       "809           currently mario shadyside handing koozies      Pittsburgh   \n",
       "229   thanks appreciate makeadifference addvalue tod...    Johnson City   \n",
       "1696                        shit pants stage stay tuned         Spokane   \n",
       "1707  okay astros fans weve torn rooting agree want ...            Eden   \n",
       "1690    sentiment exactly maineloves mainemendozaonkmjs         Ravenel   \n",
       "\n",
       "              admin1  cc  \n",
       "1            Vichada  CO  \n",
       "5           New York  US  \n",
       "4             Oregon  US  \n",
       "12             Texas  US  \n",
       "18          Illinois  US  \n",
       "...              ...  ..  \n",
       "809     Pennsylvania  US  \n",
       "229        Tennessee  US  \n",
       "1696      Washington  US  \n",
       "1707           Texas  US  \n",
       "1690  South Carolina  US  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.head(1000)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df.timestamp_ms, unit='ms').dt.date\n",
    "df['groups'] = (df.date.diff().dt.days > 1).cumsum()\n",
    "df = df.drop(df[df.groups != 0].index)\n",
    "df = df.drop(['date', 'groups'], axis=1)\n",
    "df.sort_values(by=['timestamp_ms'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = '%d\\n%.8f\\n%.8f\\n%s\\n%s\\n%s\\n%s'\n",
    "#np.savetxt(r'dataset85257EnUS.txt', df.values, fmt=fmt, delimiter='\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = '%d\\n%.8f\\n%.8f\\n%s\\n%s\\n%s\\n%s'\n",
    "df.to_csv(r'1000EnForSynth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
